{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6T2XZIkuRMF"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xQoImMiagTS"
      },
      "outputs": [],
      "source": [
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "sc = SparkContext('local')\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3WvnWn7NjXX"
      },
      "outputs": [],
      "source": [
        "# Libraries for data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Libraries for data visualisation\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn-colorblind')\n",
        "\n",
        "# Libraries for building classifiers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpC7bmd6bayg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnw501DnUKIu"
      },
      "outputs": [],
      "source": [
        "\n",
        "file_location2 = \"/content/drive/MyDrive/PREDICT GENDER/fist_name_full.csv\"\n",
        "file_location = \"/content/drive/MyDrive/PREDICT GENDER/complete_names_full.csv\"\n",
        "file_location1 = \"/content/drive/MyDrive/PREDICT GENDER/names2predict_31_mayo.csv\"\n",
        "file_type = \"csv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjkK1ykkULuF"
      },
      "outputs": [],
      "source": [
        "\n",
        "## To train and test                         \n",
        "nd_02 = spark.read.format(file_type).option(\"inferSchema\", \"true\").option(\"header\",\"true\").option(\"delimiter\",\";\").load(file_location)\n",
        "nd_02 = nd_02.withColumn(\"origen\", lit(0))\n",
        "#nd_02 = nd_02.withColumn(\"ID\", monotonically_increasing_id())\n",
        "nd_02 = nd_02.select('name', 'gender','origen')\n",
        "\n",
        "nd_01 = spark.read.format(file_type).option(\"inferSchema\", \"true\").option(\"header\",\"true\").option(\"delimiter\",\";\").load(file_location2)\n",
        "nd_01 = nd_01.withColumn(\"origen\", lit(0))\n",
        "#nd_01 = nd_01.withColumn(\"ID\", monotonically_increasing_id())\n",
        "nd_01 = nd_01.select('name', 'gender','origen')\n",
        "\n",
        "nd_0 = nd_02.union(nd_01)\n",
        "nd_0 = nd_0.withColumn(\"ID\", monotonically_increasing_id())\n",
        "nd_0 = nd_0.select('ID','name', 'gender','origen')\n",
        "nd_0 = nd_0.sample(fraction=0.07)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5eWtEBUUL2N"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql.functions import lit, StringType\n",
        "file_type = \"csv\"\n",
        "## To predict\n",
        "nd_2 = spark.read.format(file_type).option(\"inferSchema\", \"true\").option(\"header\",\"true\").option(\"delimiter\",\";\").load(file_location1)\n",
        "nd_2 = nd_2.withColumn(\"origen\", lit(2))\n",
        "nd_2 = nd_2.withColumn(\"gender\", lit(None).cast(StringType()))\n",
        "nd_2 = nd_2.select('ID','name', 'gender','origen')\n",
        "#nd_2 = nd_2.limit(2000000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBbRvMvYUL7E",
        "outputId": "f264e9b4-c97e-4970-b541-5e3c29973adf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "280545\n",
            "1835093\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(nd_0.count())\n",
        "print(nd_2.count())\n",
        "#5329203"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLuHrzD2UXqG",
        "outputId": "577ce540-d446-424f-e2d7-ab7091413729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2115638\n"
          ]
        }
      ],
      "source": [
        "nd_1 = nd_0.union(nd_2)\n",
        "print(nd_1.count())\n",
        "#nd_1 = nd_1.filter(nd_1.origen == 0).drop(nd_1.origen)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngQPJR-vUX1U",
        "outputId": "666d627d-e4ee-4c87-89f9-551f4bcdfdd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   gender   count\n",
            "0  female  166386\n",
            "1    male  114159\n",
            "+------+-----------------+\n",
            "|gender|           weight|\n",
            "+------+-----------------+\n",
            "|  male|1.228746747956797|\n",
            "|  male|1.228746747956797|\n",
            "|  male|1.228746747956797|\n",
            "+------+-----------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check imbalance and compute weights\n",
        "import pandas as pd\n",
        "counts = nd_1.filter(nd_1.origen == 0).groupBy('gender').count().toPandas()\n",
        "print(counts)\n",
        "\n",
        "\n",
        "# Counts\n",
        "count_male = counts[counts['gender']=='male']['count'].values[0]\n",
        "count_total = counts['count'].sum()\n",
        "\n",
        "# Weights\n",
        "c = 2\n",
        "weight_male = count_total / (c * count_male)\n",
        "weight_female = count_total / (c * (count_total - count_male))\n",
        "\n",
        "# Append weights to the dataset\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "#nd_1 = nd_1.withColumn(\"weight\", when(col(\"gender\") =='male', weight_male).otherwise(when(col(\"gender\") =='female', weight_female).otherwise(lit(0))\n",
        "\n",
        "nd_1 = nd_1.withColumn(\"weight\", when(nd_1.gender == \"male\",weight_male)\n",
        "                                 .when(nd_1.gender == \"female\",weight_female)\n",
        "                                 .when(nd_1.gender.isNull() ,0)\n",
        "                                 .otherwise(lit(0))) \n",
        "                                                                                   \n",
        "                                                                                     \n",
        "\n",
        "# Check everything seems ok\n",
        "nd_1.select('gender', 'weight').where(col('gender')=='male').show(3)\n",
        "\n",
        "\n",
        "ndf = nd_1.select(\"*\").toPandas()\n",
        "ndf['name'] = ndf['name'].apply(lambda x: x.lower())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07tEFtjHUX6O"
      },
      "outputs": [],
      "source": [
        "# MAGIC **Creating a new feature on name length**\n",
        "ndf['name_len'] = ndf['name'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "# MAGIC **Creating a new feature on last letter of the name**\n",
        "ndf['last_letter_vowel'] = ndf['name'].apply(lambda x: 1 if x[-1] in ['a','e','i','o','u'] else 0)\n",
        "\n",
        "\n",
        "# MAGIC **Open Vowel**\n",
        "\n",
        "ndf['open_vowel'] = ndf['name'].apply(lambda x: 1 if x[-1] in ['a','e','o'] else (2 if x[-1] in ['i','u'] else 0))\n",
        "ndf['vowel_a'] = ndf['name'].apply(lambda x: 1 if x[-1] in ['a']  else 0)\n",
        "ndf['vowel_oe'] = ndf['name'].apply(lambda x: 1 if x[-1] in ['o','e']  else 0)\n",
        "ndf['last_three'] = ndf['name'].apply(lambda x:  x[-3:])\n",
        "ndf['last_five'] = ndf['name'].apply(lambda x:  x[-5:])\n",
        "ndf['first3'] = ndf['name'].apply(lambda x:  x[:3])\n",
        "ndf['first5'] = ndf['name'].apply(lambda x:  x[:5])\n",
        "\n",
        "# MAGIC **Creating a new feature for calculating the number of vowels and consonents in a name**\n",
        "\n",
        "# MAGIC Helper function for consonent and vowel calculation\n",
        "\n",
        "\n",
        "def letter_class(name):\n",
        "    name_list = [x for x in name]\n",
        "    vowel_counter = 0\n",
        "    consonent_counter = 0\n",
        "    for letter in name_list:\n",
        "        if letter in ['a','e','i','o','u']:\n",
        "            vowel_counter+=1\n",
        "        else:\n",
        "            consonent_counter+=1\n",
        "    \n",
        "    return vowel_counter, consonent_counter\n",
        "\n",
        "\n",
        "def name_convertor(name_list):\n",
        "    ndf = pd.DataFrame([], columns=['name','name_len'\n",
        "                                    ,'last_letter_vowel','open_vowel',\n",
        "                                    'vowel_a','vowel_o', 'weight','vowel_n','last_three','last_five','first3','first5'])\n",
        "    ndf['name'] = name_list\n",
        "    ndf['name_len'] = ndf['name'].apply(lambda x: len(x))\n",
        "    ndf['last_letter_vowel'] = ndf['name'].apply(lambda x: 1 if x[-1] in ['a','e','i','o','u'] else 0)\n",
        "    ndf['open_vowel'] = ndf['name'].apply(lambda x: 1 if x[-1] in ['a','e','o'] else 0)\n",
        "    ndf['vowel_a'] = ndf['name'].apply(lambda x: 1 if x[-1] in ['a']  else 0)\n",
        "    ndf['vowel_oe'] = ndf['name'].apply(lambda x: 1 if x[-1] in ['o','e']  else 0)\n",
        "    ndf['last_three'] = ndf['name'].apply(lambda x:  x[-3:])\n",
        "    ndf['last_five'] = ndf['name'].apply(lambda x:  x[-5:])\n",
        "    ndf['first3'] = ndf['name'].apply(lambda x:  x[:3])\n",
        "    ndf['first5'] = ndf['name'].apply(lambda x:  x[:5])\n",
        "    \n",
        "    return ndf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70k-R0mnYh0g",
        "outputId": "54c6d10b-90ba-41dd-dbc7-2343704d3fbe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2115638, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# MAGIC **Encoding the gender as binary values**\n",
        "ndf['class'] = ndf['gender'].apply(lambda x: 1 if x=='female' else (0 if x == 'male' else 2))\n",
        "\n",
        "\n",
        "dataset = ndf[['name_len', 'origen','last_letter_vowel','vowel_a','vowel_oe','open_vowel','weight','class','last_three','first3','first5','last_five']]\n",
        "\n",
        "\n",
        "dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yel7JfZol0E"
      },
      "outputs": [],
      "source": [
        "# Create a Spark DataFrame from a pandas DataFrame using Arrow\n",
        "dataset_spark = spark.createDataFrame(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agv_2j8joxY9"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "last_three_indexer = StringIndexer(inputCol=\"last_three\", outputCol=\"last_threeIndex\")\n",
        "last_five_indexer = StringIndexer(inputCol=\"last_five\", outputCol=\"last_fiveIndex\")\n",
        "first3_indexer = StringIndexer(inputCol=\"first3\", outputCol=\"first3Index\")\n",
        "first5_indexer = StringIndexer(inputCol=\"first5\", outputCol=\"first5Index\")\n",
        "#Fits a model to the input dataset with optional parameters.\n",
        "dataset_spark = last_three_indexer.fit(dataset_spark).transform(dataset_spark)\n",
        "dataset_spark = last_five_indexer.fit(dataset_spark).transform(dataset_spark)\n",
        "dataset_spark = first3_indexer.fit(dataset_spark).transform(dataset_spark)\n",
        "dataset_spark = first5_indexer.fit(dataset_spark).transform(dataset_spark)\n",
        "#dataset_spark.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KseH7T7o2Se"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.ml.feature import OneHotEncoder\n",
        "\n",
        "#onehotencoder to each variable\n",
        "\n",
        "onehotencoder_last_three_vector = OneHotEncoder(inputCol=\"last_threeIndex\", outputCol=\"last_three_vec\")\n",
        "onehotencoder_last_five_vector = OneHotEncoder(inputCol=\"last_fiveIndex\", outputCol=\"last_five_vec\")\n",
        "onehotencoder_first3_vector = OneHotEncoder(inputCol=\"first3Index\", outputCol=\"first3_vec\")\n",
        "onehotencoder_first5_vector = OneHotEncoder(inputCol=\"first5Index\", outputCol=\"first5_vec\")\n",
        "\n",
        "dataset_spark = onehotencoder_last_three_vector.fit(dataset_spark).transform(dataset_spark)\n",
        "dataset_spark = onehotencoder_last_five_vector.fit(dataset_spark).transform(dataset_spark)\n",
        "dataset_spark = onehotencoder_first3_vector.fit(dataset_spark).transform(dataset_spark)\n",
        "dataset_spark = onehotencoder_first5_vector.fit(dataset_spark).transform(dataset_spark)\n",
        "\n",
        "#dataset_spark.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsabHoG6UYA-"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql.functions import log\n",
        "## Variables para filtrar los datos y para meter al modelo\n",
        "#variables = [\"COD_DANE\",\"label\",\"IPM_obs_cero\",\"IPM\",\"P1\",\"P2\",\"P3\",\"P4\",\"P5\",'REG_DEF','CLASE',\"features\"]\n",
        "#covariables = [\"P1\",\"P2\",\"P3\",\"P4\",\"P5\",'REG_DEF','CLASE']\n",
        "variables = ['label','name_len', 'origen','open_vowel','vowel_a','vowel_oe','weight','last_letter_vowel',\"features\",\n",
        "             'last_three_vec','last_five_vec','first3_vec', 'first5_vec']\n",
        "variables_new = ['name_len', 'origen','open_vowel','vowel_a','vowel_oe','last_letter_vowel',\"features\",'last_five_vec',\n",
        "             'last_three_vec','first3_vec','first5_vec']\n",
        "\n",
        "covariables = ['name_len','open_vowel','vowel_a','vowel_oe','last_letter_vowel','last_five_vec',\n",
        "              'last_three_vec','first3_vec','first5_vec']\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWlZZqdZr_Ol"
      },
      "outputs": [],
      "source": [
        "## Vectorizar las covariables que van a ingresar al modelo\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "import math\n",
        "from pyspark.sql.types import IntegerType,DoubleType\n",
        "\n",
        "definitivo = dataset_spark.withColumnRenamed(\"class\", \"label\")\n",
        "#definitivo = definitivo.filter((definitivo.origen == 0)).filter((definitivo.name_len > 2))\n",
        "definitivo = VectorAssembler(inputCols = covariables, outputCol=\"features\").setHandleInvalid(\"keep\").transform(definitivo)\n",
        "\n",
        "## Botar valores nulos en la respuesta \n",
        "datos2 = definitivo.filter(definitivo.name_len >= 4).filter(definitivo.origen == 0).sample(withReplacement=False, fraction=0.3)\n",
        "datos2 = datos2.select(variables)\n",
        "\n",
        "datos3 = definitivo.filter(definitivo.origen == 2)\n",
        "datos3 = datos3.select(variables_new)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qpF0ap3o5m7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a32955-96e4-46ec-fe86-13c141873320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1835093\n",
            "84073\n"
          ]
        }
      ],
      "source": [
        "print(datos3.count())\n",
        "print(datos2.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "bMssSzCduHRE",
        "outputId": "72eec1a7-257c-45a9-8416-b8ec03b47e21"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[label: bigint, name_len: bigint, origen: bigint, open_vowel: bigint, vowel_a: bigint, vowel_oe: bigint, weight: double, last_letter_vowel: bigint, features: vector, last_three_vec: vector, last_five_vec: vector, first3_vec: vector, first5_vec: vector]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(datos2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXGahBXUsI87"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorIndexer\n",
        "# Fit on whole dataset to include all labels in index.\n",
        "   \n",
        "labelIndexer_t= StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(datos2)\n",
        "\n",
        "#labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
        "\n",
        "featureIndexer_t =VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(datos2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ksG6bwXnNdv"
      },
      "outputs": [],
      "source": [
        "# Automatically identify categorical features, and index them.\n",
        "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
        "# featureIndexer =\\\n",
        "#    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories = 4).fit(datos2)\n",
        "\n",
        "datos2 = datos2.filter(datos2.name_len >= 2).filter(datos2.origen == 0).drop(datos2.origen)\n",
        "datos3 = datos3.filter(datos3.origen == 2).drop(datos3.origen)\n",
        "\n",
        "\n",
        "print(datos3.count())\n",
        "\n",
        "## Dividir conjunto de entrenamiento y de prueba\n",
        "\n",
        "(trainSet_g1, testSet_g1) = datos2.randomSplit([0.8, 0.2], seed=100)\n",
        "print(trainSet_g1.count())\n",
        "print(testSet_g1.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCGDeVcDT8QO"
      },
      "outputs": [],
      "source": [
        "#=====================================================================================================#\n",
        "#        Validación Cruzada para identificar los parámetros óptimos usando Random Forest              #\n",
        "#=====================================================================================================#\n",
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import numpy as np\n",
        "from pyspark.ml.feature import IndexToString\n",
        "\n",
        "## Profundidad de los árboles\n",
        "deep =[5,10,15,20,25,29]#range(2, 29)\n",
        "\n",
        "## Número de árboles\n",
        "arboles =[10, 100,200]#\n",
        "#arboles =[1,2,5,10,20,50,75,100,150,200]\n",
        "\n",
        "## Listas vacías para llenar con los valores de accuracy y árboles\n",
        "scores_1 = []\n",
        "scores = []\n",
        "arb = []\n",
        "prof = []\n",
        "\n",
        "## Semilla para dividir datos de prueba y entrenamiento\n",
        "# semillas = np.random.randint(low = 1000, high = 5000, size = len(deep)*len(arboles) )\n",
        "\n",
        "contador = 0\n",
        "for k in deep:\n",
        "  for a in arboles:\n",
        "    ## Dividir datos en conjunto de prueba y entrenamiento\n",
        "    # (trainSet_g1, testSet_g1) = datos2.randomSplit([0.8, 0.2], seed = semillas[contador])\n",
        "  \n",
        "    ## Ajuste del modelo  \n",
        "    model = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees = a, maxDepth = k, weightCol='weight', \n",
        "                                   impurity = 'entropy').fit(trainSet_g1)\n",
        "    \n",
        "    # Convert indexed labels back to original labels.\n",
        "    labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer_t.labels)\n",
        "    \n",
        "    # Chain indexers and forest in a Pipeline\n",
        "    pipeline = Pipeline(stages=[labelIndexer_t, featureIndexer_t, model, labelConverter])\n",
        "    \n",
        "    # Train model.  This also runs the indexers.\n",
        "    model = pipeline.fit(trainSet_g1)\n",
        "\n",
        "    ## Predicción en el conjunto de entrenamiento\n",
        "    predictions = model.transform(trainSet_g1)\n",
        "\n",
        "    ## Comparar contra lo observado a través del RMSE (en datos de entrenamiento)\n",
        "    evaluator = MulticlassClassificationEvaluator( labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "    accuracy = evaluator.evaluate(predictions)\n",
        "    scores.append(accuracy)\n",
        "  \n",
        "    ## Predicción en el conjunto de prueba\n",
        "    prediction_1 = model.transform(testSet_g1)\n",
        "    \n",
        "    ## Comparar contra lo observado a través del accuracy (en datos de prueba)\n",
        "    accuracy_1 = evaluator.evaluate(prediction_1)\n",
        "    scores_1.append(accuracy_1)\n",
        "    \n",
        "    ## Vector de árboles y profundidad\n",
        "    arb.append(a)\n",
        "    prof.append(k)\n",
        "    \n",
        "    ## Contador\n",
        "    contador = contador + 1\n",
        "    \n",
        "error_rf = sqlContext.createDataFrame(zip(arb, prof, scores, scores_1), schema = ['árboles','prof','accuracy_train','accuracy_test'])\n",
        "display(error_rf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSKRuFwgYYvx"
      },
      "outputs": [],
      "source": [
        "#=============================================================================#\n",
        "#      Ajuste del modelo Random Forest usando los parámetros óptimos          #\n",
        "#=============================================================================#\n",
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import numpy as np\n",
        "from pyspark.ml.feature import IndexToString\n",
        "import sys\n",
        "import os\n",
        "\n",
        "## Semilla para dividir datos de prueba y entrenamiento\n",
        "# semillas = np.random.randint(low = 1000, high = 5000, size = len(deep)*len(arboles) )\n",
        "\n",
        "## Dividir datos en conjunto de prueba y entrenamiento\n",
        "# (trainSet_g1, testSet_g1) = datos2.randomSplit([0.8, 0.2], seed = semillas[contador])\n",
        "  \n",
        "## Ajuste del modelo  \n",
        "model = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees = 100, maxDepth = 29, weightCol='weight', \n",
        "                                   impurity = 'entropy').fit(trainSet_g1)\n",
        "    \n",
        "# Convert indexed labels back to original labels.\n",
        "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer_t.labels)\n",
        "    \n",
        "# Chain indexers and forest in a Pipeline\n",
        "pipeline = Pipeline(stages=[labelIndexer_t, featureIndexer_t, model, labelConverter])\n",
        "    \n",
        "# Train model.  This also runs the indexers.\n",
        "model = pipeline.fit(trainSet_g1)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X42WM5_pKKOd"
      },
      "outputs": [],
      "source": [
        "# Step 4: Save the model\n",
        "model.save_weights('Gender.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXTVYX2bYTj0"
      },
      "outputs": [],
      "source": [
        "!cp -r 'Gender.h5'  '/content/drive/MyDrive/PREDICT GENDER/MODELGender' #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1CKpi0kstKH"
      },
      "outputs": [],
      "source": [
        "!cp -r '/content/drive/MyDrive/PREDICT GENDER/MODEL' 'boyorgirl.h5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvbjAvszKMAz"
      },
      "outputs": [],
      "source": [
        "## Predicción en el conjunto de entrenamiento y todos los datos\n",
        "predictions_train = model.transform(trainSet_g1)\n",
        "predictions_full = model.transform(datos2)\n",
        "prediction_test = model.transform(testSet_g1)\n",
        "\n",
        "\n",
        "## Comparar contra lo observado a través del Accuracy (en datos de entrenamiento)\n",
        "evaluator = MulticlassClassificationEvaluator( labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_train = evaluator.evaluate(predictions_train)\n",
        "accuracy_test = evaluator.evaluate(prediction_test)\n",
        "accuracy_full = evaluator.evaluate(predictions_full)\n",
        "    \n",
        "print(\"Accuracy in training data = %g\" % accuracy_train)\n",
        "print(\"Accuracy in testing data = %g\" % accuracy_test)\n",
        "print(\"Accuracy in full data = %g\" % accuracy_full)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vsLTGhh5aBc"
      },
      "outputs": [],
      "source": [
        "##Load model\n",
        "#import keras\n",
        "#model = keras.models.load_model(')\n",
        "# Create a basic model instance\n",
        "\n",
        "model.load_weights('/content/drive/MyDrive/PREDICT GENDER/MODEL/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLZf0qAPYJT8"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "## Confusion matrix for training data\n",
        "\n",
        "y_true = predictions_train.select(['label']).collect()\n",
        "y_pred = predictions_train.select(['prediction']).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBgSdrF-YMPi"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(y_true, y_pred))\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "df = pd.DataFrame({'y_Actual':y_true, 'y_Predicted':y_pred})\n",
        "confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], normalize='index', rownames=['Actual'], colnames=['Predicted'])\n",
        "\n",
        "sn.heatmap(confusion_matrix,fmt='.2%', cmap='Blues', annot=True)     \n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-TFyXfQYQm7"
      },
      "outputs": [],
      "source": [
        "## Confusion matrix for testing data\n",
        "\n",
        "y_true = prediction_test.select(['label']).collect()\n",
        "y_pred = prediction_test.select(['prediction']).collect()\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "df = pd.DataFrame({'y_Actual':y_true, 'y_Predicted':y_pred})\n",
        "confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], normalize='index', rownames=['Actual'], colnames=['Predicted'])\n",
        "\n",
        "sn.heatmap(confusion_matrix,fmt='.2%', cmap='Blues', annot=True)     \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgaZqJ3_YTqB"
      },
      "outputs": [],
      "source": [
        "## Confusion matrix for full data\n",
        "\n",
        "y_true = predictions_full.select(['label']).collect()\n",
        "y_pred = predictions_full.select(['prediction']).collect()\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "\n",
        "df = pd.DataFrame({'y_Actual':y_true, 'y_Predicted':y_pred})\n",
        "confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], normalize='index', rownames=['Actual'], colnames=['Predicted'])\n",
        "\n",
        "sn.heatmap(confusion_matrix,fmt='.2%', cmap='Blues', annot=True)     \n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1nvLY_Quo0q"
      },
      "outputs": [],
      "source": [
        "# import required modules\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.layers import GlobalMaxPooling2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVNZADravGPu"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSWpyiUTtSwv"
      },
      "outputs": [],
      "source": [
        "\n",
        "from keras.models import load_model\n",
        "model=load_weights(\"boyorgirl.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD0t-9wPpIto"
      },
      "outputs": [],
      "source": [
        "# MAGIC # Predicting on a new data set\n",
        "\n",
        "\n",
        "## Predicción en el conjunto de entrenamiento y todos los datos\n",
        "y_pred = model.transform(datos3).select('prediction').collect()\n",
        "\n",
        "\n",
        "df = pd.DataFrame({'y_Predicted':y_pred})\n",
        "df2= nd_2.select(\"*\").toPandas()\n",
        "\n",
        "\n",
        "df_c = pd.concat([df2, df], axis=1)\n",
        "df_c\n",
        "\n",
        "\n",
        "df_c[df_c.name == 'JOSECcarELESTINO']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhVV_Tc7V9ud"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_c.to_csv(\"sex_predited.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lym_9xP9apPJ"
      },
      "outputs": [],
      "source": [
        "!cp -r 'sex_predited.csv'  '/content/drive/MyDrive/PREDICT GENDER/sex_predited_Mig.csv' #"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}